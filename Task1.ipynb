{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "CN: 3794 images\n",
      "MCI: 473 images\n",
      "EMCI: 4320 images\n",
      "LMCI: 2656 images\n",
      "AD: 2253 images\n",
      "\n",
      "Testing Data:\n",
      "CN: 956 images\n",
      "MCI: 125 images\n",
      "EMCI: 1040 images\n",
      "LMCI: 698 images\n",
      "AD: 555 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_images_in_directory(main_dir):\n",
    "    image_files = [f for f in os.listdir(main_dir) if f.endswith(\".dcm\")]\n",
    "\n",
    "    class_counts = {}\n",
    "\n",
    "    for image_file in image_files:\n",
    "        class_label = image_file.split(\"_\")[-1].split(\".\")[0]\n",
    "\n",
    "        if class_label in class_counts:\n",
    "            class_counts[class_label] += 1\n",
    "        else:\n",
    "            class_counts[class_label] = 1\n",
    "\n",
    "    return class_counts\n",
    "\n",
    "main_dir_training = r\"D:\\Research\\Neurodegenerative Diseases\\Images\\Task1\\Task1-Images\\MRI\\Training\"\n",
    "main_dir_testing = r\"D:\\Research\\Neurodegenerative Diseases\\Images\\Task1\\Task1-Images\\MRI\\Testing\"\n",
    "\n",
    "print(\"Training Data:\")\n",
    "training_counts = count_images_in_directory(main_dir_training)\n",
    "for class_label, count in training_counts.items():\n",
    "    print(f\"{class_label}: {count} images\")\n",
    "\n",
    "print(\"\\nTesting Data:\")\n",
    "testing_counts = count_images_in_directory(main_dir_testing)\n",
    "for class_label, count in testing_counts.items():\n",
    "    print(f\"{class_label}: {count} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "CN: 13583 images\n",
      "MCI: 7458 images\n",
      "EMCI: 11646 images\n",
      "LMCI: 3524 images\n",
      "AD: 3301 images\n",
      "\n",
      "Testing Data:\n",
      "CN: 3360 images\n",
      "MCI: 1971 images\n",
      "EMCI: 2865 images\n",
      "LMCI: 896 images\n",
      "AD: 786 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_images_in_directory(main_dir):\n",
    "    image_files = [f for f in os.listdir(main_dir) if f.endswith(\".dcm\")]\n",
    "\n",
    "    class_counts = {}\n",
    "\n",
    "    for image_file in image_files:\n",
    "        class_label = image_file.split(\"_\")[-1].split(\".\")[0]\n",
    "\n",
    "        if class_label in class_counts:\n",
    "            class_counts[class_label] += 1\n",
    "        else:\n",
    "            class_counts[class_label] = 1\n",
    "\n",
    "    return class_counts\n",
    "\n",
    "main_dir_training = r\"D:\\Research\\Neurodegenerative Diseases\\Images\\Task1\\Task1-Images\\PET\\Training\"\n",
    "main_dir_testing = r\"D:\\Research\\Neurodegenerative Diseases\\Images\\Task1\\Task1-Images\\PET\\Testing\"\n",
    "\n",
    "print(\"Training Data:\")\n",
    "training_counts = count_images_in_directory(main_dir_training)\n",
    "for class_label, count in training_counts.items():\n",
    "    print(f\"{class_label}: {count} images\")\n",
    "\n",
    "print(\"\\nTesting Data:\")\n",
    "testing_counts = count_images_in_directory(main_dir_testing)\n",
    "for class_label, count in testing_counts.items():\n",
    "    print(f\"{class_label}: {count} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "CN: 34430 images\n",
      "MCI: 4773 images\n",
      "EMCI: 38491 images\n",
      "LMCI: 25573 images\n",
      "AD: 24885 images\n",
      "\n",
      "Testing Data:\n",
      "CN: 8613 images\n",
      "MCI: 1233 images\n",
      "EMCI: 9557 images\n",
      "LMCI: 6490 images\n",
      "AD: 6146 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_images_in_directory(main_dir):\n",
    "    image_files = [f for f in os.listdir(main_dir) if f.endswith(\".dcm\")]\n",
    "\n",
    "    class_counts = {}\n",
    "\n",
    "    for image_file in image_files:\n",
    "        class_label = image_file.split(\"_\")[-1].split(\".\")[0]\n",
    "\n",
    "        if class_label in class_counts:\n",
    "            class_counts[class_label] += 1\n",
    "        else:\n",
    "            class_counts[class_label] = 1\n",
    "\n",
    "    return class_counts\n",
    "\n",
    "main_dir_training = r\"D:\\Research\\Neurodegenerative Diseases\\Images\\Task1\\Task1-Images\\fMRI\\Training\"\n",
    "main_dir_testing = r\"D:\\Research\\Neurodegenerative Diseases\\Images\\Task1\\Task1-Images\\fMRI\\Testing\"\n",
    "\n",
    "print(\"Training Data:\")\n",
    "training_counts = count_images_in_directory(main_dir_training)\n",
    "for class_label, count in training_counts.items():\n",
    "    print(f\"{class_label}: {count} images\")\n",
    "\n",
    "print(\"\\nTesting Data:\")\n",
    "testing_counts = count_images_in_directory(main_dir_testing)\n",
    "for class_label, count in testing_counts.items():\n",
    "    print(f\"{class_label}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.applications import InceptionV3, EfficientNetB3\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import pydicom\n",
    "import cv2\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "# Set your data directory, batch size, image size, number of epochs, and number of classes\n",
    "data_dir = r'D:\\Alzheimers\\Task1\\Task1-Images\\MRI\\Images'\n",
    "batch_size = 32\n",
    "image_size = (128, 128)\n",
    "num_epochs = 20\n",
    "num_classes = 5\n",
    "\n",
    "# Function to load and preprocess images and labels\n",
    "def load_and_preprocess_images_and_labels(directory):\n",
    "    image_data = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".dcm\"):\n",
    "            dcm_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                dcm_data = pydicom.dcmread(dcm_path)\n",
    "                img = dcm_data.pixel_array.astype(np.float32) / 255.0\n",
    "                img = cv2.resize(img, image_size)\n",
    "                image_data.append(img)\n",
    "                \n",
    "                filename_parts = filename.split('_')\n",
    "                class_name = filename_parts[-1].split('.')[0]\n",
    "                \n",
    "                class_mapping = {'CN': 0, 'AD': 1, 'MCI': 2, 'EMCI': 3, 'LMCI': 4}\n",
    "                label = class_mapping.get(class_name, -1) \n",
    "                \n",
    "                if label != -1:\n",
    "                    labels.append(label)\n",
    "                else:\n",
    "                    print(f\"Unknown class name: {class_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {dcm_path}: {str(e)}\")\n",
    "\n",
    "    return np.array(image_data), np.array(labels)\n",
    "\n",
    "# Load and preprocess all data from the directory\n",
    "all_images, all_labels = load_and_preprocess_images_and_labels(data_dir)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    all_images, all_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Expand grayscale images to three channels (RGB)\n",
    "train_images = np.repeat(train_images[:, :, :, np.newaxis], 3, axis=-1)\n",
    "test_images = np.repeat(test_images[:, :, :, np.newaxis], 3, axis=-1)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes)\n",
    "\n",
    "# Calculate class weights to handle class imbalance\n",
    "class_weights = class_weight.compute_sample_weight('balanced', train_labels.argmax(axis=1))\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Create a data generator for training images with more augmentation\n",
    "train_data_generator = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Define a custom callback to save the best model based on the best validation accuracy across all trials\n",
    "class SaveBestModel(Callback):\n",
    "    def __init__(self, filepath, monitor='val_accuracy', mode='max'):\n",
    "        super(SaveBestModel, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.best_accuracy = float('-inf') if mode == 'max' else float('inf')\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_accuracy = logs.get(self.monitor)\n",
    "        if current_accuracy is None:\n",
    "            return\n",
    "\n",
    "        if (self.mode == 'max' and current_accuracy > self.best_accuracy) or (self.mode == 'min' and current_accuracy < self.best_accuracy):\n",
    "            self.best_accuracy = current_accuracy\n",
    "            self.model.save(self.filepath, overwrite=True)\n",
    "            print(f\"Saved best model with {self.monitor} {current_accuracy:.4f}\")\n",
    "\n",
    "# Define a function to create and compile a model\n",
    "def build_model(hp):\n",
    "    # Choose a transfer learning model architecture\n",
    "    transfer_model = hp.Choice('transfer_model', ['EfficientNetB3'])\n",
    "    if transfer_model == 'EfficientNetB3':\n",
    "        base_model = EfficientNetB3(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Add regularization (L2 regularization)\n",
    "    x = Dense(1024, activation='relu', kernel_regularizer=l2(hp.Float('regularization', 1e-5, 1e-3, sampling='log')))(x)\n",
    "    \n",
    "    # Add dropout for regularization\n",
    "    x = Dropout(hp.Float('dropout', 0.2, 0.5, step=0.1))(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Freeze some layers of the base model\n",
    "    for layer in base_model.layers[:15]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Compile the model with a variable learning rate\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Float('learning_rate', 1e-5, 1e-2, sampling='log')),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Instantiate the RandomSearch tuner\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,  # Number of hyperparameter combinations to try\n",
    "    directory='tuner_results',  # Directory to store results\n",
    "    project_name='alzheimer'  # Name of the project\n",
    ")\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "tuner.search(train_images, train_labels, epochs=num_epochs, validation_split=0.2, class_weight=class_weights, callbacks=[early_stopping])\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Save the best model\n",
    "checkpoint_filepath = 'MRI_Task1.h5'\n",
    "best_model.save(checkpoint_filepath)\n",
    "\n",
    "# Evaluate the best model on the test dataset\n",
    "test_loss, test_accuracy = best_model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Save the model at the end of training\n",
    "end_model_filepath = 'End_MRI'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1234/1234 [==============================] - 2628s 2s/step - loss: 0.8465 - accuracy: 0.4468\n",
      "Epoch 2/20\n",
      "1234/1234 [==============================] - 2596s 2s/step - loss: 0.7028 - accuracy: 0.5389\n",
      "Epoch 3/20\n",
      "1234/1234 [==============================] - 2597s 2s/step - loss: 0.5907 - accuracy: 0.6128\n",
      "Epoch 4/20\n",
      "1234/1234 [==============================] - 2597s 2s/step - loss: 0.4941 - accuracy: 0.6786\n",
      "Epoch 5/20\n",
      "1234/1234 [==============================] - 2600s 2s/step - loss: 0.4092 - accuracy: 0.7433\n",
      "Epoch 6/20\n",
      "1234/1234 [==============================] - 2596s 2s/step - loss: 0.3458 - accuracy: 0.7851\n",
      "Epoch 7/20\n",
      "1234/1234 [==============================] - 2588s 2s/step - loss: 0.2983 - accuracy: 0.8178\n",
      "Epoch 8/20\n",
      "1234/1234 [==============================] - 2599s 2s/step - loss: 0.2588 - accuracy: 0.8432\n",
      "Epoch 9/20\n",
      "1234/1234 [==============================] - 2610s 2s/step - loss: 0.2279 - accuracy: 0.8620\n",
      "Epoch 10/20\n",
      "1234/1234 [==============================] - 2608s 2s/step - loss: 0.2107 - accuracy: 0.8749\n",
      "Epoch 11/20\n",
      "1234/1234 [==============================] - 2601s 2s/step - loss: 0.1936 - accuracy: 0.8857\n",
      "Epoch 12/20\n",
      "1234/1234 [==============================] - 2587s 2s/step - loss: 0.1777 - accuracy: 0.8948\n",
      "Epoch 13/20\n",
      "1234/1234 [==============================] - 2593s 2s/step - loss: 0.1667 - accuracy: 0.9016\n",
      "Epoch 14/20\n",
      "1234/1234 [==============================] - 2593s 2s/step - loss: 0.1574 - accuracy: 0.9092\n",
      "Epoch 15/20\n",
      "1234/1234 [==============================] - 2604s 2s/step - loss: 0.1486 - accuracy: 0.9134\n",
      "Epoch 16/20\n",
      "1234/1234 [==============================] - 2594s 2s/step - loss: 0.1428 - accuracy: 0.9169\n",
      "Epoch 17/20\n",
      "1234/1234 [==============================] - 2586s 2s/step - loss: 0.1353 - accuracy: 0.9206\n",
      "Epoch 18/20\n",
      "1234/1234 [==============================] - 2586s 2s/step - loss: 0.1293 - accuracy: 0.9251\n",
      "Epoch 19/20\n",
      "1234/1234 [==============================] - 2587s 2s/step - loss: 0.1230 - accuracy: 0.9286\n",
      "Epoch 20/20\n",
      "1234/1234 [==============================] - 2585s 2s/step - loss: 0.1214 - accuracy: 0.9301\n",
      "309/309 - 144s - loss: 0.1608 - accuracy: 0.9422 - 144s/epoch - 465ms/step\n",
      "Test Accuracy: 94.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import pydicom\n",
    "import cv2\n",
    "\n",
    "# Define your data directory, batch size, image size, number of epochs, and number of classes\n",
    "data_dir = r'D:\\Alzheimers\\Task1\\Task1-Images\\PET\\Images'\n",
    "batch_size = 32\n",
    "image_size = (128, 128)\n",
    "num_epochs = 20\n",
    "num_classes = 5\n",
    "\n",
    "def load_and_preprocess_images_and_labels(directory):\n",
    "    # Function to load and preprocess images and labels\n",
    "    image_data = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".dcm\"):\n",
    "            dcm_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                dcm_data = pydicom.dcmread(dcm_path)\n",
    "                img = dcm_data.pixel_array.astype(np.float32) / 255.0\n",
    "                img = cv2.resize(img, image_size)\n",
    "                image_data.append(img)\n",
    "\n",
    "                filename_parts = filename.split('_')\n",
    "                class_name = filename_parts[-1].split('.')[0]\n",
    "\n",
    "                class_mapping = {'CN': 0, 'AD': 1, 'MCI': 2, 'EMCI': 3, 'LMCI': 4}\n",
    "                label = class_mapping.get(class_name, -1)\n",
    "\n",
    "                if label != -1:\n",
    "                    labels.append(label)\n",
    "                else:\n",
    "                    print(f\"Unknown class name: {class_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {dcm_path}: {str(e)}\")\n",
    "\n",
    "    return np.array(image_data), np.array(labels)\n",
    "\n",
    "# Load and preprocess all data from the directory\n",
    "all_images, all_labels = load_and_preprocess_images_and_labels(data_dir)\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    all_images, all_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Expand grayscale images to three channels (RGB)\n",
    "train_images = np.repeat(train_images[:, :, :, np.newaxis], 3, axis=-1)\n",
    "test_images = np.repeat(test_images[:, :, :, np.newaxis], 3, axis=-1)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes)\n",
    "\n",
    "# Calculate class weights to handle class imbalance\n",
    "class_weights = class_weight.compute_sample_weight('balanced', train_labels.argmax(axis=1))\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Create a data generator for training images with more augmentation\n",
    "train_data_generator = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Load the EfficientNetB3 model\n",
    "base_model = EfficientNetB3(weights='imagenet', include_top=False, input_shape=(128, 128, 3))  # Use EfficientNetB3\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)  # Add dropout for regularization\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze some layers of the base model (you can experiment with the number of layers to freeze)\n",
    "for layer in base_model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model with a lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the data generator\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    history = model.fit(\n",
    "        train_data_generator.flow(train_images, train_labels, batch_size=batch_size),\n",
    "        steps_per_epoch=len(train_images) // batch_size,\n",
    "        epochs=1,\n",
    "        class_weight=class_weights,  # Apply class weights\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Define the path where you want to save the model\n",
    "save_path = r'D:\\Alzheimers\\Models\\PET_Task1.h5'  \n",
    "\n",
    "# Save the model to the specified location\n",
    "model.save(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "16019/16019 [==============================] - 2777s 173ms/step - loss: 0.5988 - accuracy: 0.7300\n",
      "Epoch 2/5\n",
      "16019/16019 [==============================] - 2845s 178ms/step - loss: 0.1452 - accuracy: 0.9457\n",
      "Epoch 3/5\n",
      "16019/16019 [==============================] - 2850s 178ms/step - loss: 0.0900 - accuracy: 0.9684\n",
      "Epoch 4/5\n",
      "16019/16019 [==============================] - 2836s 177ms/step - loss: 0.0680 - accuracy: 0.9770\n",
      "Epoch 5/5\n",
      "16019/16019 [==============================] - 2849s 178ms/step - loss: 0.0610 - accuracy: 0.9800\n",
      "1002/1002 - 257s - loss: 0.0117 - accuracy: 0.9968 - 257s/epoch - 257ms/step\n",
      "Test Accuracy: 99.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16, InceptionV3\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import pydicom\n",
    "import cv2\n",
    "\n",
    "data_dir = r'D:\\Alzheimers\\Task1\\Task1-Images\\fMRI\\Images'\n",
    "batch_size = 8\n",
    "image_size = (75, 75)\n",
    "num_epochs = 5  \n",
    "num_classes = 5  \n",
    "\n",
    "def load_and_preprocess_images_and_labels(directory):\n",
    "    image_data = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".dcm\"):\n",
    "            dcm_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                dcm_data = pydicom.dcmread(dcm_path)\n",
    "                img = dcm_data.pixel_array.astype(np.float32) / 255.0\n",
    "                img = cv2.resize(img, image_size)\n",
    "                image_data.append(img)\n",
    "                \n",
    "                filename_parts = filename.split('_')\n",
    "                class_name = filename_parts[-1].split('.')[0]\n",
    "                \n",
    "                class_mapping = {'CN': 0, 'AD': 1, 'MCI': 2, 'EMCI': 3, 'LMCI': 4}\n",
    "                label = class_mapping.get(class_name, -1) \n",
    "                \n",
    "                if label != -1:\n",
    "                    labels.append(label)\n",
    "                else:\n",
    "                    print(f\"Unknown class name: {class_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {dcm_path}: {str(e)}\")\n",
    "\n",
    "    return np.array(image_data), np.array(labels)\n",
    "\n",
    "\n",
    "# Load and preprocess all data from the directory\n",
    "all_images, all_labels = load_and_preprocess_images_and_labels(data_dir)\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    all_images, all_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Expand grayscale images to three channels (RGB)\n",
    "train_images = np.repeat(train_images[:, :, :, np.newaxis], 3, axis=-1)\n",
    "test_images = np.repeat(test_images[:, :, :, np.newaxis], 3, axis=-1)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes)\n",
    "\n",
    "# Calculate class weights to handle class imbalance\n",
    "class_weights = class_weight.compute_sample_weight('balanced', train_labels.argmax(axis=1))\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Create a data generator for training images with more augmentation\n",
    "train_data_generator = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Load the VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(75, 75, 3))\n",
    "# or, alternatively, use InceptionV3\n",
    "# base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)  # Add dropout for regularization\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze some layers of the base model (you can experiment with the number of layers to freeze)\n",
    "for layer in base_model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model with a lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the data generator\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    history = model.fit(\n",
    "        train_data_generator.flow(train_images, train_labels, batch_size=batch_size),\n",
    "        steps_per_epoch=len(train_images) // batch_size,\n",
    "        epochs=1,\n",
    "        class_weight=class_weights,  # Apply class weights\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Define the path where you want to save the model\n",
    "save_path = r'D:\\Alzheimers\\Models\\fMRI_Task1.h5'  # Replace with your desired path\n",
    "\n",
    "# Save the model to the specified location\n",
    "model.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "528/528 [==============================] - 281s 528ms/step\n",
      "Test Accuracy: 89.12270302311796 %\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pydicom\n",
    "import cv2\n",
    "\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('MRI_Task1.h5')  # Replace with the actual path to your saved model\n",
    "\n",
    "# Define data directory and image size\n",
    "data_dir = r'D:\\Alzheimers\\Task1\\Task1-Images\\MRI\\Images'\n",
    "image_size = (128, 128)  # Adjust this to match the input size your model expects\n",
    "\n",
    "# Create a function to load and preprocess test images\n",
    "def load_and_preprocess_test_images(directory):\n",
    "    image_data = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".dcm\"):\n",
    "            dcm_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                dcm_data = pydicom.dcmread(dcm_path)\n",
    "                img = dcm_data.pixel_array.astype(np.float32) / 255.0\n",
    "                img = cv2.resize(img, image_size)\n",
    "                image_data.append(img)\n",
    "                \n",
    "                filename_parts = filename.split('_')\n",
    "                class_name = filename_parts[-1].split('.')[0]\n",
    "                \n",
    "                class_mapping = {'CN': 0, 'AD': 1, 'MCI': 2, 'EMCI': 3, 'LMCI': 4}\n",
    "                label = class_mapping.get(class_name, -1) \n",
    "                \n",
    "                if label != -1:\n",
    "                    labels.append(label)\n",
    "                else:\n",
    "                    print(f\"Unknown class name: {class_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {dcm_path}: {str(e)}\")\n",
    "\n",
    "    return np.array(image_data), np.array(labels)\n",
    "\n",
    "# Load and preprocess the test images\n",
    "test_images, true_labels = load_and_preprocess_test_images(data_dir)\n",
    "\n",
    "# Expand grayscale images to three channels (RGB)\n",
    "test_images = np.repeat(test_images[:, :, :, np.newaxis], 3, axis=-1)\n",
    "\n",
    "# Use the model to make predictions on the test data\n",
    "predictions = model.predict(test_images)\n",
    "\n",
    "# Obtain predicted labels (class indices with highest probability)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy by comparing predicted and true labels\n",
    "accuracy = np.mean(np.equal(predicted_labels, true_labels))\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Test Accuracy:\", accuracy * 100, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pydicom\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Function to load, preprocess images, and assign labels\n",
    "def load_and_preprocess_images_with_labels(directory, image_size):\n",
    "    image_data = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".dcm\"):\n",
    "            dcm_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                dcm_data = pydicom.dcmread(dcm_path)\n",
    "                img = dcm_data.pixel_array.astype(np.float32) / 255.0\n",
    "                img = cv2.resize(img, image_size)\n",
    "                image_data.append(img)\n",
    "                filename_parts = filename.split('_')\n",
    "                class_name = filename_parts[-1].split('.')[0]\n",
    "                class_mapping = {'CN': 0, 'AD': 1, 'MCI': 2, 'EMCI': 3, 'LMCI': 4}\n",
    "                label = class_mapping.get(class_name, -1)\n",
    "                if label != -1:\n",
    "                    labels.append(label)\n",
    "                else:\n",
    "                    print(f\"Unknown class name: {class_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {dcm_path}: {str(e)}\")\n",
    "\n",
    "    return np.array(image_data), np.array(labels)\n",
    "\n",
    "data_dir = r'D:\\Research\\Neurodegenerative Diseases\\Images\\Task1\\Voting' \n",
    "\n",
    "image_size_fmri = (75, 75)  \n",
    "image_size_mri = (128, 128) \n",
    "image_size_pet = (128, 128) \n",
    "\n",
    "test_data, true_labels = load_and_preprocess_images_with_labels(data_dir, image_size)\n",
    "\n",
    "model_fmri = load_model('D:/Research/Neurodegenerative Diseases/Models/fMRI_Task1.h5')\n",
    "model_mri = load_model('D:/Research/Neurodegenerative Diseases/Models/MRI_Task1.h5')\n",
    "model_pet = load_model('D:/Research/Neurodegenerative Diseases/Models/PET_Task1.h5')\n",
    "\n",
    "test_data = np.repeat(test_data[:, :, :, np.newaxis], 3, axis=-1) \n",
    "\n",
    "\n",
    "\n",
    "test_data_fmri_resized = []\n",
    "for image in test_data:\n",
    "    resized_image = cv2.resize(image, (75, 75))  \n",
    "    test_data_fmri_resized.append(resized_image)\n",
    "test_data_fmri_resized = np.array(test_data_fmri_resized)\n",
    "print(test_data_fmri_resized.shape) \n",
    "\n",
    "predictions_fmri = model_fmri.predict(test_data_fmri_resized)\n",
    "\n",
    "\n",
    "\n",
    "test_data_mri_resized = []\n",
    "for image in test_data:\n",
    "    resized_image = cv2.resize(image, (128, 128)) \n",
    "    test_data_mri_resized.append(resized_image)\n",
    "test_data_mri_resized = np.array(test_data_mri_resized)\n",
    "print(test_data_mri_resized.shape) \n",
    "\n",
    "predictions_mri = model_mri.predict(test_data_mri_resized)\n",
    "\n",
    "\n",
    "\n",
    "test_data_pet_resized = [] \n",
    "for image in test_data:\n",
    "    resized_image = cv2.resize(image, (128, 128))\n",
    "    test_data_pet_resized.append(resized_image)\n",
    "test_data_pet_resized = np.array(test_data_pet_resized)\n",
    "print(test_data_pet_resized.shape) \n",
    "\n",
    "predictions_pet = model_pet.predict(test_data_pet_resized)\n",
    "\n",
    "\n",
    "def majority_vote(predictions_fmri, predictions_mri, predictions_pet):\n",
    "    predictions = []\n",
    "    for i in range(len(predictions_fmri)):\n",
    "        combined_prediction = np.argmax([predictions_fmri[i], predictions_mri[i], predictions_pet[i]])\n",
    "        predictions.append(combined_prediction % 5)  \n",
    "    return predictions\n",
    "\n",
    "final_predictions = majority_vote(predictions_fmri, predictions_mri, predictions_pet)\n",
    "\n",
    "accuracy = accuracy_score(true_labels, final_predictions)\n",
    "\n",
    "predicted_labels_mri = np.argmax(predictions_mri, axis=1)\n",
    "predicted_labels_fmri = np.argmax(predictions_fmri, axis=1)\n",
    "predicted_labels_pet = np.argmax(predictions_pet, axis=1)\n",
    "\n",
    "# print(\"MRI Predicted Class Labels:\", predicted_labels_mri)\n",
    "# print(\"fMRI Predicted Class Labels:\", predicted_labels_fmri)\n",
    "# print(\"PET Predicted Class Labels:\", predicted_labels_pet)\n",
    "\n",
    "# print(\"Actual Labels:\", true_labels)\n",
    "# print(\"Final Predictions (Majority Voting):\", final_predictions)\n",
    "print(f\"Voting Classifier Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
